# pykili_project

### Описание ###

Лингвистическое исследование на основе собранного вручную корпуса эпистолярных романов и сообщений из соц. сетей (vk,tg)

### Как работает код и как его запускать ###

code:

1. extract_texts.py - принимает папку с файлами, извлекает тексты и года из книг (txt) и архивов чатов ВК (html) и Телеграм (json), сам сортирует файлы в зависимости от расширения, возвращает списки сообщений с годами отправки
2. parser.py - принимает файл в формате json из предыдущего шага и делает морфологическую разметку каждого слова, возвращает также json
3. connector.py - принимает папку с фалами json, может соединить несколько файлов с морфологической разметкой из пункта (2) или несколько файлов с распределением прилагательных из пункта (4), возвращает один файл json
4. research_1.py - принимает на вход файл json с морфологической разметкой и сортирует прилагательные по годам, возвращает словарь json
5. research_2.py - принимает на вход файл json с морфологической разметкой и сортирует части речи по годам, так же возвращает словарь json

visualization:

1. counter.py - принимает json файл с морфологической разметкой (результат extract_texts.py + parser.py) и возвращает json файл с посчитанной частотностью слов (слова идут от самого частого использования до самого редкого, напротив них количество употреблений)
2. spreadsheet.py - принимает json и делает из него таблицу (принимает результат counter.py - делает из него таблицу частотностей, принимает результат research_1.py - делает из него таблицу с существительными и прилагательными к ним, распределенными по годам)
3. bar.py - принимает на вход часть research_2.py и делает из него таблицу (часть - это отрывок json файла с конкретной частью речью, то есть NOUN или VERB)
        - проблема: не удалось сделать код, который принимал бы research_2.py целиком и сразу делал из него таблицу с частями речи, частотностью их употреблений и годам, пришлось делать какой-то объем работы вручную

results:

1. booksadjandnoun.xlsx и booksadjandnoun.xlsx - результаты research_1.py и spreadsheet.py - таблица, распределяющая прилагательные, употребляемые с конктретным существительным, по годам
2. pos_books и pos_socials - результат research_2.py, json файл, содержит части речи и частотности их употребления в разных годах (по книгам и социальным сетям соответственно)
3. freqlistsheetbooks.csv и freqlistsocials.csv - результат extract_text.py + parser.py + counter.py - таблицы (соответственно по книгам и социальным сетям) с "рейтингом" употребляемых слов по частоте
4. quantbooks и quantsocials - таблицы с частями речи и их частотностью по годам (книги и социальные сети соответственно)

### План проекта ###

1. Собрать данные, опросить знакомых, обкачать паблики: 
    - чаты мессенджеров 
        - проблема: авторское право, конфиденциальность личных данных
    - книги в эпистолярном жанре

2. Написать код дбработки данных, конкретно для извлечения текстов и извлечения годов:
    - книги в формате txt
    - телеграм чаты в формате json
    - ВК чаты в формате html
        - проблема: ну знаете регулрку напишешь, а потом окажется, что все равно нашел не все, поэтому регулярки совершенствовались по мере нахождения новых штук
        - проблема: баг с регуляркой для дат, решился более сложной регуляркой (на самом деле не до конца решился)
3. Морфопарсер, написать код, создающий список слов с морфологическим разбором (начальной формой, частью речи, некоторыми грамматическими категориями, имеющими значениями для именных категорий, т.е. существительных и прилагательных: падеж, число, род)
    - проблема: опечатки, несуществующие слова, сленг (например, "лол кек" вообще непонятно как парсить)
    - решение: если останутся время силы, можно сделать базу данных наиболее популярных штук, которые нельзя распарсить стандартным парсером, например регулярками находить все варианты "ахахах", "пфф" или те же "лол кек"
4. Лингвистическое исследование, написать несколько алгоритмов сортировки распарсенных данных:
    - Анализ сочетаемости прилагательных с существительными
        - проблема: сложность с обработкой большого количества данных
        - решение: разбиение на части, дополнительный код, соединяющий части вместе
        - проблема: хотя наш алгоритм действительно находит достаточно много прилагательных к существительным, но свободный порядок слов в русском языке мешает найти все случаи
    - Подсчет количества частей речи по годам в процентах
    - Список частотных слов по годам

### Используемые модули ###

- os
- re
- json
- string
- pymorphy
- pandas

### Участники ###

- Софья Шакирова, БКЛ-202 - сбор данных (соц. сети, книги), коды обработки данных (папка code), морфопарсер
- Полина Карпова, БКЛ-204 - сбор данных (книги), визуализация (папка visualization), исследование, презентация 
- Татьяна Ковтун, БКЛ-204 - сбор данных (книги), исследование, презентация
